{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ObjectDetection(RETINA_NET).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-aavWXCGdLO","executionInfo":{"status":"ok","timestamp":1618485488226,"user_tz":-330,"elapsed":1265,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}},"outputId":"a509ca4a-0434-4ce4-d30d-71ea120602d9"},"source":["import os\n","import re\n","import zipfile\n","\n","import numpy as np\n","import tensorflow as tf\n","print(tf.__version__)\n","from tensorflow import keras\n","\n","import matplotlib.pyplot as plt\n","import tensorflow_datasets as tfds"],"execution_count":6,"outputs":[{"output_type":"stream","text":["2.4.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4H1y8nXH0FeO","executionInfo":{"status":"ok","timestamp":1618485497988,"user_tz":-330,"elapsed":10198,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}},"outputId":"31db6ca3-5734-46d8-f33e-982bbf319025"},"source":["pip install pycoco\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Collecting pycoco\n","  Downloading https://files.pythonhosted.org/packages/ed/b1/8a4e0663e2c9184fc9c800d4691d98cd86ffa21f2b2aeec39bec4386a5a8/pycoco-0.7.2.tar.bz2\n","Collecting ll-xist>=3.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/49/41d0c7ad99b3672b929601c934cba777efebf09aa7f5170a61dcfb455e93/ll-xist-5.65.tar.gz (729kB)\n","\u001b[K     |████████████████████████████████| 737kB 12.4MB/s \n","\u001b[?25hCollecting cssutils==1.0.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/15/a9fb9010f58d1c55dd0b7779db2334feb9a572d407024f39a60f44293861/cssutils-1.0.2-py3-none-any.whl (406kB)\n","\u001b[K     |████████████████████████████████| 409kB 30.5MB/s \n","\u001b[?25hBuilding wheels for collected packages: pycoco, ll-xist\n","  Building wheel for pycoco (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycoco: filename=pycoco-0.7.2-cp37-none-any.whl size=9791 sha256=a72ced1179cceb8f6adc1efb85a978230652bc4a5cab140b080a66dd10705164\n","  Stored in directory: /root/.cache/pip/wheels/d5/9f/9c/6f40b261f0abad9f29cfe967547d036e4bcfa6a225e442a678\n","  Building wheel for ll-xist (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ll-xist: filename=ll_xist-5.65-cp37-cp37m-linux_x86_64.whl size=635977 sha256=3f3d75de3c597f6a8112b07c8dc3402b190f183ab251b94f25a921abb844fbfd\n","  Stored in directory: /root/.cache/pip/wheels/87/0b/ed/f5cc37aaddb7e4468737921ddf47bf8b96ad4543ceefdcbfd7\n","Successfully built pycoco ll-xist\n","Installing collected packages: cssutils, ll-xist, pycoco\n","Successfully installed cssutils-1.0.2 ll-xist-5.65 pycoco-0.7.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bN28-1Ds0gf6","executionInfo":{"status":"ok","timestamp":1618485522773,"user_tz":-330,"elapsed":1522,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["from pycocotools.coco import COCO"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_EjaBk0oquG"},"source":["/content/data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRs_5BbsGgG3","executionInfo":{"status":"ok","timestamp":1618485561237,"user_tz":-330,"elapsed":6518,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["url = \"https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\"\n","filename = os.path.join(os.getcwd(), \"data.zip\")\n","keras.utils.get_file(filename, url)\n","\n","\n","with zipfile.ZipFile(\"data.zip\", \"r\") as z_fp:\n","    z_fp.extractall(\"./\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqIwpgif-JE_","executionInfo":{"status":"ok","timestamp":1618485572486,"user_tz":-330,"elapsed":5433,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}},"outputId":"e5948230-10b9-49d9-9e5e-56be49e98d00"},"source":["!unzip /content/data.zip"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Archive:  /content/data.zip\n","replace data/checkpoint? [y]es, [n]o, [A]ll, [N]one, [r]ename: "],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iMns1ddgGkKS","executionInfo":{"status":"ok","timestamp":1618485576538,"user_tz":-330,"elapsed":1460,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["def swap_xy(boxes):\n","    \"\"\"Swaps order the of x and y coordinates of the boxes.\n","\n","    Arguments:\n","      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n","\n","    Returns:\n","      swapped boxes with shape same as that of boxes.\n","    \"\"\"\n","    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n","\n","\n","def convert_to_xywh(boxes):\n","    \"\"\"Changes the box format to center, width and height.\n","\n","    Arguments:\n","      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n","        representing bounding boxes where each box is of the format\n","        `[xmin, ymin, xmax, ymax]`.\n","\n","    Returns:\n","      converted boxes with shape same as that of boxes.\n","    \"\"\"\n","    return tf.concat(\n","        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n","        axis=-1,\n","    )\n","\n","\n","def convert_to_corners(boxes):\n","    \"\"\"Changes the box format to corner coordinates\n","\n","    Arguments:\n","      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n","        representing bounding boxes where each box is of the format\n","        `[x, y, width, height]`.\n","\n","    Returns:\n","      converted boxes with shape same as that of boxes.\n","    \"\"\"\n","    return tf.concat(\n","        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n","        axis=-1,\n","    )"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"LxSV1MyKGpTr","executionInfo":{"status":"ok","timestamp":1618485579415,"user_tz":-330,"elapsed":2030,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["def compute_iou(boxes1, boxes2):\n","    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n","\n","    Arguments:\n","      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n","        where each box is of the format `[x, y, width, height]`.\n","        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n","        where each box is of the format `[x, y, width, height]`.\n","\n","    Returns:\n","      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n","        jth column holds the IOU between ith box and jth box from\n","        boxes1 and boxes2 respectively.\n","    \"\"\"\n","    boxes1_corners = convert_to_corners(boxes1)\n","    boxes2_corners = convert_to_corners(boxes2)\n","    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n","    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n","    intersection = tf.maximum(0.0, rd - lu)\n","    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n","    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n","    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n","    union_area = tf.maximum(\n","        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n","    )\n","    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)\n","\n","\n","def visualize_detections(\n","    image, boxes, classes, scores, figsize=(7, 7), linewidth=1, color=[0, 0, 1]\n","):\n","    \"\"\"Visualize Detections\"\"\"\n","    image = np.array(image, dtype=np.uint8)\n","    plt.figure(figsize=figsize)\n","    plt.axis(\"off\")\n","    plt.imshow(image)\n","    ax = plt.gca()\n","    for box, _cls, score in zip(boxes, classes, scores):\n","        text = \"{}: {:.2f}\".format(_cls, score)\n","        x1, y1, x2, y2 = box\n","        w, h = x2 - x1, y2 - y1\n","        patch = plt.Rectangle(\n","            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n","        )\n","        ax.add_patch(patch)\n","        ax.text(\n","            x1,\n","            y1,\n","            text,\n","            bbox={\"facecolor\": color, \"alpha\": 0.4},\n","            clip_box=ax.clipbox,\n","            clip_on=True,\n","        )\n","    plt.show()\n","    return ax"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYTxsdnuI4Dm","executionInfo":{"status":"ok","timestamp":1618485582178,"user_tz":-330,"elapsed":1879,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["class AnchorBox:\n","    \"\"\"Generates anchor boxes.\n","\n","    This class has operations to generate anchor boxes for feature maps at\n","    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n","    format `[x, y, width, height]`.\n","\n","    Attributes:\n","      aspect_ratios: A list of float values representing the aspect ratios of\n","        the anchor boxes at each location on the feature map\n","      scales: A list of float values representing the scale of the anchor boxes\n","        at each location on the feature map.\n","      num_anchors: The number of anchor boxes at each location on feature map\n","      areas: A list of float values representing the areas of the anchor\n","        boxes for each feature map in the feature pyramid.\n","      strides: A list of float value representing the strides for each feature\n","        map in the feature pyramid.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.aspect_ratios = [0.5, 1.0, 2.0]\n","        self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n","\n","        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n","        self._strides = [2 ** i for i in range(3, 8)]\n","        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 256.0, 512.0]]\n","        self._anchor_dims = self._compute_dims()\n","\n","    def _compute_dims(self):\n","        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n","        of the feature pyramid.\n","        \"\"\"\n","        anchor_dims_all = []\n","        for area in self._areas:\n","            anchor_dims = []\n","            for ratio in self.aspect_ratios:\n","                anchor_height = tf.math.sqrt(area / ratio)\n","                anchor_width = area / anchor_height\n","                dims = tf.reshape(\n","                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n","                )\n","                for scale in self.scales:\n","                    anchor_dims.append(scale * dims)\n","            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n","        return anchor_dims_all\n","\n","    def _get_anchors(self, feature_height, feature_width, level):\n","        \"\"\"Generates anchor boxes for a given feature map size and level\n","\n","        Arguments:\n","          feature_height: An integer representing the height of the feature map.\n","          feature_width: An integer representing the width of the feature map.\n","          level: An integer representing the level of the feature map in the\n","            feature pyramid.\n","\n","        Returns:\n","          anchor boxes with the shape\n","          `(feature_height * feature_width * num_anchors, 4)`\n","        \"\"\"\n","        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n","        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n","        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n","        centers = tf.expand_dims(centers, axis=-2)\n","        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n","        dims = tf.tile(\n","            self._anchor_dims[level - 3], [feature_height, feature_width, 1, 1]\n","        )\n","        anchors = tf.concat([centers, dims], axis=-1)\n","        return tf.reshape(\n","            anchors, [feature_height * feature_width * self._num_anchors, 4]\n","        )\n","\n","    def get_anchors(self, image_height, image_width):\n","        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n","\n","        Arguments:\n","          image_height: Height of the input image.\n","          image_width: Width of the input image.\n","\n","        Returns:\n","          anchor boxes for all the feature maps, stacked as a single tensor\n","            with shape `(total_anchors, 4)`\n","        \"\"\"\n","        anchors = [\n","            self._get_anchors(\n","                tf.math.ceil(image_height / 2 ** i),\n","                tf.math.ceil(image_width / 2 ** i),\n","                i,\n","            )\n","            for i in range(3, 8)\n","        ]\n","        return tf.concat(anchors, axis=0)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oq363-eeGt5I","executionInfo":{"status":"ok","timestamp":1618485584579,"user_tz":-330,"elapsed":1308,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["def random_flip_horizontal(image, boxes):\n","    \"\"\"Flips image and boxes horizontally with 50% chance\n","\n","    Arguments:\n","      image: A 3-D tensor of shape `(height, width, channels)` representing an\n","        image.\n","      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n","        having normalized coordinates.\n","\n","    Returns:\n","      Randomly flipped image and boxes\n","    \"\"\"\n","    if tf.random.uniform(()) > 0.5:\n","        image = tf.image.flip_left_right(image)\n","        boxes = tf.stack(\n","            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\n","        )\n","    return image, boxes\n","\n","\n","def resize_and_pad_image(\n","    image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n","):\n","    \"\"\"Resizes and pads image while preserving aspect ratio.\n","\n","    1. Resizes images so that the shorter side is equal to `min_side`\n","    2. If the longer side is greater than `max_side`, then resize the image\n","      with longer side equal to `max_side`\n","    3. Pad with zeros on right and bottom to make the image shape divisible by\n","    `stride`\n","\n","    Arguments:\n","      image: A 3-D tensor of shape `(height, width, channels)` representing an\n","        image.\n","      min_side: The shorter side of the image is resized to this value, if\n","        `jitter` is set to None.\n","      max_side: If the longer side of the image exceeds this value after\n","        resizing, the image is resized such that the longer side now equals to\n","        this value.\n","      jitter: A list of floats containing minimum and maximum size for scale\n","        jittering. If available, the shorter side of the image will be\n","        resized to a random value in this range.\n","      stride: The stride of the smallest feature map in the feature pyramid.\n","        Can be calculated using `image_size / feature_map_size`.\n","\n","    Returns:\n","      image: Resized and padded image.\n","      image_shape: Shape of the image before padding.\n","      ratio: The scaling factor used to resize the image\n","    \"\"\"\n","    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n","    if jitter is not None:\n","        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n","    ratio = min_side / tf.reduce_min(image_shape)\n","    if ratio * tf.reduce_max(image_shape) > max_side:\n","        ratio = max_side / tf.reduce_max(image_shape)\n","    image_shape = ratio * image_shape\n","    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n","    padded_image_shape = tf.cast(\n","        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n","    )\n","    image = tf.image.pad_to_bounding_box(\n","        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n","    )\n","    return image, image_shape, ratio\n","\n","\n","def preprocess_data(sample):\n","    \"\"\"Applies preprocessing step to a single sample\n","\n","    Arguments:\n","      sample: A dict representing a single training sample.\n","\n","    Returns:\n","      image: Resized and padded image with random horizontal flipping applied.\n","      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n","        of the format `[x, y, width, height]`.\n","      class_id: An tensor representing the class id of the objects, having\n","        shape `(num_objects,)`.\n","    \"\"\"\n","    image = sample[\"image\"]\n","    bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n","    class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\n","\n","    image, bbox = random_flip_horizontal(image, bbox)\n","    image, image_shape, _ = resize_and_pad_image(image)\n","\n","    bbox = tf.stack(\n","        [\n","            bbox[:, 0] * image_shape[1],\n","            bbox[:, 1] * image_shape[0],\n","            bbox[:, 2] * image_shape[1],\n","            bbox[:, 3] * image_shape[0],\n","        ],\n","        axis=-1,\n","    )\n","    bbox = convert_to_xywh(bbox)\n","    return image, bbox, class_id"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"M996ulHtG6eL","executionInfo":{"status":"ok","timestamp":1618485587742,"user_tz":-330,"elapsed":2338,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["class LabelEncoder:\n","    \"\"\"Transforms the raw labels into targets for training.\n","\n","    This class has operations to generate targets for a batch of samples which\n","    is made up of the input images, bounding boxes for the objects present and\n","    their class ids.\n","\n","    Attributes:\n","      anchor_box: Anchor box generator to encode the bounding boxes.\n","      box_variance: The scaling factors used to scale the bounding box targets.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self._anchor_box = AnchorBox()\n","        self._box_variance = tf.convert_to_tensor(\n","            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n","        )\n","\n","    def _match_anchor_boxes(\n","        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.4\n","    ):\n","        \"\"\"Matches ground truth boxes to anchor boxes based on IOU.\n","\n","        1. Calculates the pairwise IOU for the M `anchor_boxes` and N `gt_boxes`\n","          to get a `(M, N)` shaped matrix.\n","        2. The ground truth box with the maximum IOU in each row is assigned to\n","          the anchor box provided the IOU is greater than `match_iou`.\n","        3. If the maximum IOU in a row is less than `ignore_iou`, the anchor\n","          box is assigned with the background class.\n","        4. The remaining anchor boxes that do not have any class assigned are\n","          ignored during training.\n","\n","        Arguments:\n","          anchor_boxes: A float tensor with the shape `(total_anchors, 4)`\n","            representing all the anchor boxes for a given input image shape,\n","            where each anchor box is of the format `[x, y, width, height]`.\n","          gt_boxes: A float tensor with shape `(num_objects, 4)` representing\n","            the ground truth boxes, where each box is of the format\n","            `[x, y, width, height]`.\n","          match_iou: A float value representing the minimum IOU threshold for\n","            determining if a ground truth box can be assigned to an anchor box.\n","          ignore_iou: A float value representing the IOU threshold under which\n","            an anchor box is assigned to the background class.\n","\n","        Returns:\n","          matched_gt_idx: Index of the matched object\n","          positive_mask: A mask for anchor boxes that have been assigned ground\n","            truth boxes.\n","          ignore_mask: A mask for anchor boxes that need to by ignored during\n","            training\n","        \"\"\"\n","        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n","        max_iou = tf.reduce_max(iou_matrix, axis=1)\n","        matched_gt_idx = tf.argmax(iou_matrix, axis=1)\n","        positive_mask = tf.greater_equal(max_iou, match_iou)\n","        negative_mask = tf.less(max_iou, ignore_iou)\n","        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))\n","        return (\n","            matched_gt_idx,\n","            tf.cast(positive_mask, dtype=tf.float32),\n","            tf.cast(ignore_mask, dtype=tf.float32),\n","        )\n","\n","    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n","        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n","        box_target = tf.concat(\n","            [\n","                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n","                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n","            ],\n","            axis=-1,\n","        )\n","        box_target = box_target / self._box_variance\n","        return box_target\n","\n","    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n","        \"\"\"Creates box and classification targets for a single sample\"\"\"\n","        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n","        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n","        matched_gt_idx, positive_mask, ignore_mask = self._match_anchor_boxes(\n","            anchor_boxes, gt_boxes\n","        )\n","        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n","        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)\n","        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n","        cls_target = tf.where(\n","            tf.not_equal(positive_mask, 1.0), -1.0, matched_gt_cls_ids\n","        )\n","        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -2.0, cls_target)\n","        cls_target = tf.expand_dims(cls_target, axis=-1)\n","        label = tf.concat([box_target, cls_target], axis=-1)\n","        return label\n","\n","    def encode_batch(self, batch_images, gt_boxes, cls_ids):\n","        \"\"\"Creates box and classification targets for a batch\"\"\"\n","        images_shape = tf.shape(batch_images)\n","        batch_size = images_shape[0]\n","\n","        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n","        for i in range(batch_size):\n","            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n","            labels = labels.write(i, label)\n","        batch_images = tf.keras.applications.resnet.preprocess_input(batch_images)\n","        return batch_images, labels.stack()"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fp8snoiHG-vY","executionInfo":{"status":"ok","timestamp":1618485590054,"user_tz":-330,"elapsed":1457,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["def get_backbone():\n","    \"\"\"Builds ResNet50 with pre-trained imagenet weights\"\"\"\n","    backbone = keras.applications.ResNet50(\n","        include_top=False, input_shape=[None, None, 3]\n","    )\n","    c3_output, c4_output, c5_output = [\n","        backbone.get_layer(layer_name).output\n","        for layer_name in [\"conv3_block4_out\", \"conv4_block6_out\", \"conv5_block3_out\"]\n","    ]\n","    return keras.Model(\n","        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n","    )"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"cjh9ByIAHB5G","executionInfo":{"status":"ok","timestamp":1618485592519,"user_tz":-330,"elapsed":1530,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["class FeaturePyramid(keras.layers.Layer):\n","    \"\"\"Builds the Feature Pyramid with the feature maps from the backbone.\n","\n","    Attributes:\n","      num_classes: Number of classes in the dataset.\n","      backbone: The backbone to build the feature pyramid from.\n","        Currently supports ResNet50 only.\n","    \"\"\"\n","\n","    def __init__(self, backbone=None, **kwargs):\n","        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n","        self.backbone = backbone if backbone else get_backbone()\n","        self.conv_c3_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n","        self.conv_c4_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n","        self.conv_c5_1x1 = keras.layers.Conv2D(256, 1, 1, \"same\")\n","        self.conv_c3_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n","        self.conv_c4_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n","        self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, \"same\")\n","        self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n","        self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, \"same\")\n","        self.upsample_2x = keras.layers.UpSampling2D(2)\n","\n","    def call(self, images, training=False):\n","        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n","        p3_output = self.conv_c3_1x1(c3_output)\n","        p4_output = self.conv_c4_1x1(c4_output)\n","        p5_output = self.conv_c5_1x1(c5_output)\n","        p4_output = p4_output + self.upsample_2x(p5_output)\n","        p3_output = p3_output + self.upsample_2x(p4_output)\n","        p3_output = self.conv_c3_3x3(p3_output)\n","        p4_output = self.conv_c4_3x3(p4_output)\n","        p5_output = self.conv_c5_3x3(p5_output)\n","        p6_output = self.conv_c6_3x3(c5_output)\n","        p7_output = self.conv_c7_3x3(tf.nn.relu(p6_output))\n","        return p3_output, p4_output, p5_output, p6_output, p7_output"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"V21znh0CHF9L","executionInfo":{"status":"ok","timestamp":1618485594943,"user_tz":-330,"elapsed":1550,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["def build_head(output_filters, bias_init):\n","    \"\"\"Builds the class/box predictions head.\n","\n","    Arguments:\n","      output_filters: Number of convolution filters in the final layer.\n","      bias_init: Bias Initializer for the final convolution layer.\n","\n","    Returns:\n","      A keras sequential model representing either the classification\n","        or the box regression head depending on `output_filters`.\n","    \"\"\"\n","    head = keras.Sequential([keras.Input(shape=[None, None, 256])])\n","    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n","    for _ in range(4):\n","        head.add(\n","            keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n","        )\n","        head.add(keras.layers.ReLU())\n","    head.add(\n","        keras.layers.Conv2D(\n","            output_filters,\n","            3,\n","            1,\n","            padding=\"same\",\n","            kernel_initializer=kernel_init,\n","            bias_initializer=bias_init,\n","        )\n","    )\n","    return head"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3ssoXPeHNch","executionInfo":{"status":"ok","timestamp":1618485597665,"user_tz":-330,"elapsed":1584,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["class RetinaNet(keras.Model):\n","    \"\"\"A subclassed Keras model implementing the RetinaNet architecture.\n","\n","    Attributes:\n","      num_classes: Number of classes in the dataset.\n","      backbone: The backbone to build the feature pyramid from.\n","        Currently supports ResNet50 only.\n","    \"\"\"\n","\n","    def __init__(self, num_classes, backbone=None, **kwargs):\n","        super(RetinaNet, self).__init__(name=\"RetinaNet\", **kwargs)\n","        self.fpn = FeaturePyramid(backbone)\n","        self.num_classes = num_classes\n","\n","        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n","        self.cls_head = build_head(9 * num_classes, prior_probability)\n","        self.box_head = build_head(9 * 4, \"zeros\")\n","\n","    def call(self, image, training=False):\n","        features = self.fpn(image, training=training)\n","        N = tf.shape(image)[0]\n","        cls_outputs = []\n","        box_outputs = []\n","        for feature in features:\n","            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n","            cls_outputs.append(\n","                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n","            )\n","        cls_outputs = tf.concat(cls_outputs, axis=1)\n","        box_outputs = tf.concat(box_outputs, axis=1)\n","        return tf.concat([box_outputs, cls_outputs], axis=-1)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"VlUP1bQRHShR","executionInfo":{"status":"ok","timestamp":1618485599904,"user_tz":-330,"elapsed":1454,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["class DecodePredictions(tf.keras.layers.Layer):\n","    \"\"\"A Keras layer that decodes predictions of the RetinaNet model.\n","\n","    Attributes:\n","      num_classes: Number of classes in the dataset\n","      confidence_threshold: Minimum class probability, below which detections\n","        are pruned.\n","      nms_iou_threshold: IOU threshold for the NMS operation\n","      max_detections_per_class: Maximum number of detections to retain per\n","       class.\n","      max_detections: Maximum number of detections to retain across all\n","        classes.\n","      box_variance: The scaling factors used to scale the bounding box\n","        predictions.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        num_classes=80,\n","        confidence_threshold=0.05,\n","        nms_iou_threshold=0.5,\n","        max_detections_per_class=100,\n","        max_detections=100,\n","        box_variance=[0.1, 0.1, 0.2, 0.2],\n","        **kwargs\n","    ):\n","        super(DecodePredictions, self).__init__(**kwargs)\n","        self.num_classes = num_classes\n","        self.confidence_threshold = confidence_threshold\n","        self.nms_iou_threshold = nms_iou_threshold\n","        self.max_detections_per_class = max_detections_per_class\n","        self.max_detections = max_detections\n","\n","        self._anchor_box = AnchorBox()\n","        self._box_variance = tf.convert_to_tensor(\n","            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n","        )\n","\n","    def _decode_box_predictions(self, anchor_boxes, box_predictions):\n","        boxes = box_predictions * self._box_variance\n","        boxes = tf.concat(\n","            [\n","                boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n","                tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n","            ],\n","            axis=-1,\n","        )\n","        boxes_transformed = convert_to_corners(boxes)\n","        return boxes_transformed\n","\n","    def call(self, images, predictions):\n","        image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n","        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n","        box_predictions = predictions[:, :, :4]\n","        cls_predictions = tf.nn.sigmoid(predictions[:, :, 4:])\n","        boxes = self._decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n","\n","        return tf.image.combined_non_max_suppression(\n","            tf.expand_dims(boxes, axis=2),\n","            cls_predictions,\n","            self.max_detections_per_class,\n","            self.max_detections,\n","            self.nms_iou_threshold,\n","            self.confidence_threshold,\n","            clip_boxes=False,\n","        )"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"QX52lcKyHWCS","executionInfo":{"status":"ok","timestamp":1618485603381,"user_tz":-330,"elapsed":2267,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["class RetinaNetBoxLoss(tf.losses.Loss):\n","    \"\"\"Implements Smooth L1 loss\"\"\"\n","\n","    def __init__(self, delta):\n","        super(RetinaNetBoxLoss, self).__init__(\n","            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n","        )\n","        self._delta = delta\n","\n","    def call(self, y_true, y_pred):\n","        difference = y_true - y_pred\n","        absolute_difference = tf.abs(difference)\n","        squared_difference = difference ** 2\n","        loss = tf.where(\n","            tf.less(absolute_difference, self._delta),\n","            0.5 * squared_difference,\n","            absolute_difference - 0.5,\n","        )\n","        return tf.reduce_sum(loss, axis=-1)\n","\n","\n","class RetinaNetClassificationLoss(tf.losses.Loss):\n","    \"\"\"Implements Focal loss\"\"\"\n","\n","    def __init__(self, alpha, gamma):\n","        super(RetinaNetClassificationLoss, self).__init__(\n","            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n","        )\n","        self._alpha = alpha\n","        self._gamma = gamma\n","\n","    def call(self, y_true, y_pred):\n","        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n","            labels=y_true, logits=y_pred\n","        )\n","        probs = tf.nn.sigmoid(y_pred)\n","        alpha = tf.where(tf.equal(y_true, 1.0), self._alpha, (1.0 - self._alpha))\n","        pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n","        loss = alpha * tf.pow(1.0 - pt, self._gamma) * cross_entropy\n","        return tf.reduce_sum(loss, axis=-1)\n","\n","\n","class RetinaNetLoss(tf.losses.Loss):\n","    \"\"\"Wrapper to combine both the losses\"\"\"\n","\n","    def __init__(self, num_classes=80, alpha=0.25, gamma=2.0, delta=1.0):\n","        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n","        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma)\n","        self._box_loss = RetinaNetBoxLoss(delta)\n","        self._num_classes = num_classes\n","\n","    def call(self, y_true, y_pred):\n","        y_pred = tf.cast(y_pred, dtype=tf.float32)\n","        box_labels = y_true[:, :, :4]\n","        box_predictions = y_pred[:, :, :4]\n","        cls_labels = tf.one_hot(\n","            tf.cast(y_true[:, :, 4], dtype=tf.int32),\n","            depth=self._num_classes,\n","            dtype=tf.float32,\n","        )\n","        cls_predictions = y_pred[:, :, 4:]\n","        positive_mask = tf.cast(tf.greater(y_true[:, :, 4], -1.0), dtype=tf.float32)\n","        ignore_mask = tf.cast(tf.equal(y_true[:, :, 4], -2.0), dtype=tf.float32)\n","        clf_loss = self._clf_loss(cls_labels, cls_predictions)\n","        box_loss = self._box_loss(box_labels, box_predictions)\n","        clf_loss = tf.where(tf.equal(ignore_mask, 1.0), 0.0, clf_loss)\n","        box_loss = tf.where(tf.equal(positive_mask, 1.0), box_loss, 0.0)\n","        normalizer = tf.reduce_sum(positive_mask, axis=-1)\n","        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n","        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n","        loss = clf_loss + box_loss\n","        return loss"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mn-zHrjbHZR7","executionInfo":{"status":"ok","timestamp":1618485611115,"user_tz":-330,"elapsed":6861,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["model_dir = \"retinanet/\"\n","label_encoder = LabelEncoder()\n","\n","num_classes = 80\n","batch_size = 2\n","\n","learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n","learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n","learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n","    boundaries=learning_rate_boundaries, values=learning_rates\n",")"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqmrgxgnHc0R","executionInfo":{"status":"ok","timestamp":1618485616462,"user_tz":-330,"elapsed":4195,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}},"outputId":"b0edbbe4-6a73-456a-c7b2-b1a628405f2c"},"source":["resnet50_backbone = get_backbone()\n","loss_fn = RetinaNetLoss(num_classes)\n","model = RetinaNet(num_classes, resnet50_backbone)\n","\n","optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n","model.compile(loss=loss_fn, optimizer=optimizer)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94773248/94765736 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wc7tCuAaHpCs","executionInfo":{"status":"ok","timestamp":1618485641816,"user_tz":-330,"elapsed":1933,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["callbacks_list = [\n","    tf.keras.callbacks.ModelCheckpoint(\n","        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n","        monitor=\"loss\",\n","        save_best_only=False,\n","        save_weights_only=True,\n","        verbose=1,\n","    )\n","]"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5Orr4xQHrly","executionInfo":{"status":"ok","timestamp":1618485644145,"user_tz":-330,"elapsed":1397,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["(train_dataset, val_dataset), dataset_info = tfds.load(\n","    \"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir=\"data\"\n",")"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"-t5Vg7ZAHuaW","executionInfo":{"status":"ok","timestamp":1618485648436,"user_tz":-330,"elapsed":3343,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["autotune = tf.data.experimental.AUTOTUNE\n","train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n","train_dataset = train_dataset.shuffle(8 * batch_size)\n","train_dataset = train_dataset.padded_batch(\n","    batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",")\n","train_dataset = train_dataset.map(\n","    label_encoder.encode_batch, num_parallel_calls=autotune\n",")\n","train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n","train_dataset = train_dataset.prefetch(autotune)\n","\n","val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n","val_dataset = val_dataset.padded_batch(\n","    batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",")\n","val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n","val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n","val_dataset = val_dataset.prefetch(autotune)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzK1I6JaHzMw","executionInfo":{"status":"ok","timestamp":1618485834652,"user_tz":-330,"elapsed":184885,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}},"outputId":"4c52dd33-5098-4df4-e42d-e0a95c3d8758"},"source":["# Uncomment the following lines, when training on full dataset\n","# train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // batch_size\n","# val_steps_per_epoch = \\\n","#     dataset_info.splits[\"validation\"].num_examples // batch_size\n","\n","# train_steps = 4 * 100000\n","# epochs = train_steps // train_steps_per_epoch\n","\n","epochs = 1\n","\n","# Running 100 training and 50 validation steps,\n","# remove `.take` when training on the full dataset\n","\n","model.fit(\n","    train_dataset.take(100),\n","    validation_data=val_dataset.take(50),\n","    epochs=epochs,\n","    callbacks=callbacks_list,\n","    verbose=1,\n",")"],"execution_count":27,"outputs":[{"output_type":"stream","text":["100/100 [==============================] - 182s 1s/step - loss: 4.0748 - val_loss: 4.0849\n","\n","Epoch 00001: saving model to retinanet/weights_epoch_1\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f839ccfe6d0>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZk6XqqoH3dH","executionInfo":{"status":"ok","timestamp":1618485857498,"user_tz":-330,"elapsed":2189,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}},"outputId":"2f1fac59-d454-4412-bf49-b36d4d18776f"},"source":["# Change this to `model_dir` when not using the downloaded weights\n","weights_dir = \"data\"\n","\n","latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n","model.load_weights(latest_checkpoint)"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f839cd38ed0>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"Tipow4ytH6FI","executionInfo":{"status":"ok","timestamp":1618485896582,"user_tz":-330,"elapsed":3012,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}}},"source":["image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n","predictions = model(image, training=False)\n","detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n","inference_model = tf.keras.Model(inputs=image, outputs=detections)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQfNFR0YR4XM","executionInfo":{"status":"ok","timestamp":1618486040382,"user_tz":-330,"elapsed":1416,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}},"outputId":"777ec17c-e25e-4d62-b34b-220143b79a02"},"source":["print(image[0])"],"execution_count":32,"outputs":[{"output_type":"stream","text":["KerasTensor(type_spec=TensorSpec(shape=(None, None, 3), dtype=tf.float32, name=None), name='tf.__operators__.getitem_1/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_1'\")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zaa6bDX6Pzfz"},"source":["sampletest = val_dataset.take(5):\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_urrz0S3Ndwx9-5T0fmwSUxFs0vf_fB_"},"id":"LUl5f0IkH86X","executionInfo":{"status":"error","timestamp":1618486676695,"user_tz":-330,"elapsed":19883,"user":{"displayName":"Abhishant kumar","photoUrl":"","userId":"14106152887302377135"}},"outputId":"0da724de-12b2-497a-c2ba-d88de55f9cdd"},"source":["def prepare_image(image):\n","    image, _, ratio = resize_and_pad_image(image, jitter=None)\n","    image = tf.keras.applications.resnet.preprocess_input(image)\n","    return tf.expand_dims(image, axis=0), ratio\n","\n","\n","val_dataset = tfds.load(\"coco/2017\", split=\"validation\", data_dir=\"data\")\n","int2str = dataset_info.features[\"objects\"][\"label\"].int2str\n","\n","for sample in val_dataset.take(50):\n","    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n","    input_image, ratio = prepare_image(image)\n","    detections = inference_model.predict(input_image)\n","    num_detections = detections.valid_detections[0]\n","    class_names = [\n","        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n","    ]\n","    visualize_detections(\n","        image,\n","        detections.nmsed_boxes[0][:num_detections] / ratio,\n","        class_names,\n","        detections.nmsed_scores[0][:num_detections],\n","    )"],"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"Bvb_KaAMIAXs"},"source":[""],"execution_count":null,"outputs":[]}]}